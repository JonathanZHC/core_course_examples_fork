{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e8fa-a5c2-4219-a5d0-d6cc0ade876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from rest.utils import *\n",
    "\n",
    "import numpy as np\n",
    "import casadi as ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554d557",
   "metadata": {},
   "source": [
    "### **Problem setup:**\n",
    "\n",
    "- Task: start from given initial position $p_0$, to reach a given terget position $p_T$ (Stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = k \\cdot p$\n",
    "   - case 3: varying slope (nonlinear case), $h(p) = k \\cdot cos(\\omega p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = k \\cdot sin(\\omega p)$\n",
    "\n",
    "- System dynmaics of 1d mountain car model (in State space representation): \n",
    "   - state vector $\\bm{x} = [p, v]^T$\n",
    "   - input vector $u = a$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} a\n",
    "   \\end{align*}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cad49",
   "metadata": {},
   "source": [
    "### **Part (a): define the mountain car environment and the system dynamics**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi symbolic system, inclusive defining the profile over a slope $h(p)$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establish the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "**Step 1: specify the arguments for class `Env` and instantiate the class**\n",
    "\n",
    "- To start with the simpler case (also more compatible with LQR), we will initially focus on a linear system in an unconstrained scenario\n",
    "\n",
    "- Parameters in the task:  \n",
    "   - case: 1 / 2 / 3 / 4\n",
    "   \n",
    "   - initial state: $\\bm{x}_0 = [-0.5, 0.0]^T$\n",
    "   - target state: $\\bm{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "**Step 2: call function `test_env()` to plot the mountain profile $h(p)$ and curve of inclination angle $\\theta(p)$**\n",
    "\n",
    "**Step 3: specify the arguments for class `Dynmaics` and instantiate the class**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d782c95-de15-49ec-a999-37ee410d8a86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2 or 3 or 4\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.5\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "# Arguments (without constraints): \n",
    "#   1) case: $n \\in [1, 2, 3, 4]$, type: int\n",
    "#   2) initial state: x_0 = [p_0, v_0], type: np.array\n",
    "#   3) terminal state: x_T = [p_T, v_T], type: np.array\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env.test_env() #  shape of slope (left side) and theta curve (right side) \n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "# Arguments: \n",
    "#   1) an object of class `Env`, type: Env  \n",
    "dynamics = Dynamics(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f7729",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c0d4",
   "metadata": {},
   "source": [
    "### **Part (b): Implement the discrete-time finite-horizon iLQR controller**\n",
    "\n",
    "In this section, we will provide the implementation of the LQR controller and demonstrate how it can be applied to a specific task (stabilization task). We will also show the resulting performance and the effects of using the LQR controller in achieving optimal control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13ad3",
   "metadata": {},
   "source": [
    "**Recall from LQR:**\n",
    "\n",
    "For a linear system:\n",
    "\n",
    "$$\n",
    "\\bm{x}_{k+1} = \\bm{A}_k \\bm{x}_k + \\bm{B}_k \\bm{u}_k\n",
    "$$\n",
    "\n",
    "The infinite-horizon discrete-time quadratic cost function is given by:\n",
    "\n",
    "$$\n",
    "J(\\bm{x}_0) = \\sum_{k=0}^{\\infty} \\left( (\\bm{x}_k - \\bm{x}_{ref})^T \\bm{Q} (\\bm{x}_k - \\bm{x}_{ref}) + \\bm{u}_k^T \\bm{R} \\bm{u}_k \\right)\n",
    "$$\n",
    "\n",
    "The solution to this problem is obtained by solving the **Discrete Algebraic Riccati Equation (DARE):**\n",
    "\n",
    "$$\n",
    "\\bm{S} = \\bm{Q} + \\bm{A}^T \\bm{S} \\bm{A} - (\\bm{A}^T \\bm{S} \\bm{B})(\\bm{R} + \\bm{B}^T \\bm{S} \\bm{B})^{-1} (\\bm{B}^T \\bm{S} \\bm{A})\n",
    "$$\n",
    "\n",
    "The optimal feedback control policy (LQR policy) is given by:\n",
    "\n",
    "$$\n",
    "\\bm{u^*} = \\bm{K} (\\bm{x}_k - \\bm{x}_{ref}), \\quad \\bm{K} = -\\left( \\bm{R} + \\bm{B}^T \\bm{S} \\bm{B} \\right)^{-1} \\left( \\bm{B}^T \\bm{S} \\bm{A} \\right)\n",
    "$$\n",
    "\n",
    "In the last section of LQR chapter, we have already introduced the three major limitations of LQR controller: quadratic cost, linear dynamics and no constraints. In the following part of this chapter, we will introduce the iterative linear quadratic regulator (iLQR), which relax the first two limitation to a nonquadratic cost and nonlinear dynamcis case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da13586",
   "metadata": {},
   "source": [
    "**Problem Formulation of iLQR:**\n",
    "\n",
    "\n",
    "Consider a general time-invariant discrete-time nonlinear system:\n",
    "\n",
    "$$\n",
    "\\bm{x}_{k+1} = \\bm{f}(\\bm{x}_k, \\bm{u}_k)\n",
    "$$\n",
    "\n",
    "The general discrete-time cost (not necessarily in a quadratic form) with finite-horizon $N$ is given by:\n",
    "\n",
    "$$\n",
    "J(\\bm{x_0}) = g_N(\\bm{x}_N) + \\sum_{k=0}^{N-1} g_k(\\bm{x}_k, \\bm{u}_k)\n",
    "$$\n",
    "\n",
    "We want to solve the discrete-time optimal control policy for the problem defined ablove.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab1f3d",
   "metadata": {},
   "source": [
    "**iLQR Algorithm:**\n",
    "\n",
    "- **Initialization**: An initial (stabilizing) control policy $\\{\\bm{\\mu}_k^0(\\bm{x}_k)\\}_{k=0}^{N-1}$\n",
    "\n",
    "- **Recursion** $ l = \\{0, 1, ...\\} $:\n",
    "\n",
    "  1. **\"Forward pass\"**: Apply the current control policy to the nonlinear system and obtain state and input trajectories\n",
    "\n",
    "     $$\n",
    "     \\{ \\bm{\\bar{x}}_0^l, \\bm{\\bar{x}}_1^l, ..., \\bm{\\bar{x}}_N^l \\} \\quad and \\quad \\{ \\bm{\\bar{u}}_0^l, \\bm{\\bar{u}}_1^l, ..., \\bm{\\bar{u}}_{N-1}^l \\}\n",
    "     $$\n",
    "  \n",
    "  2. Initialize **\"backward pass\"**:  \n",
    "     $$\n",
    "     \\bar{s}_N = \\bar{g}_N = g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "     $$\n",
    "     \\bm{s}_N = \\bm{q}_N = \\nabla_{\\bm{x}_N} g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "     $$\n",
    "     \\bm{S}_N = \\bm{Q}_N = \\nabla^2_{\\bm{x}_N} g_N(\\bm{\\bar{x}}_N),\n",
    "     $$\n",
    "\n",
    "  3. **\"Backward pass\"** $ k = \\{N-1, N-2, ..., 0\\} $:\n",
    "\n",
    "     i) Linearize the dynamics about $(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k)$ to obtain $ \\bm{A}_k $ and $ \\bm{B}_k $ in $\\bm{\\delta x}_{k+1} = \\bm{A}_k \\bm{\\delta x}_k + \\bm{B}_k \\bm{\\delta u}_k$:\n",
    "        $$\n",
    "        \\bm{A}_k = \\nabla_{\\bm{x}_k} \\bm{f}(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{B}_k = \\nabla_{\\bm{u}_k} \\bm{f}(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        where $ \\bm{\\delta x}_k = \\bm{x}_k - \\bm{\\bar{x}}_k $, $ \\quad \\bm{\\delta x}_{k+1} = \\bm{x}_{k+1} - \\bm{\\bar{x}}_{k+1} $, $\\quad$ and $ \\bm{\\delta u}_k = \\bm{u}_k - \\bm{\\bar{u}}_k $.\n",
    "\n",
    "     ii) Approximate the stage cost $ g_k(\\bm{x}_k, \\bm{u}_k) $ with a second-order Taylor expansion about $ (\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k) $ to construct:\n",
    "        $$\n",
    "        \\bar{g}_k = g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{q}_k = \\nabla_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{Q}_k = \\nabla^2_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{r}_k = \\nabla_{\\bm{u}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{R}_k = \\nabla^2_{\\bm{u}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k),\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{P}_k = \\nabla_{\\bm{u}_k} \\left( \\nabla^T_{\\bm{x}_k} g_k(\\bm{\\bar{x}}_k, \\bm{\\bar{u}}_k) \\right),\n",
    "        $$\n",
    "\n",
    "     iii) Approximate the cost-to-go $ J_k^*(\\bm{x}_k) $ and $ J_{k+1}^*(\\bm{x}_{k+1}) $ with a second-order Taylor expansion to construct:\n",
    "        $$\n",
    "        \\bm{l}_k = \\bm{r}_k + \\bm{B}_k^T s_{k+1},\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{G}_k = \\bm{P}_k + \\bm{B}_k^T \\bm{S}_{k+1} \\bm{A}_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{H}_k = \\bm{R}_k + \\bm{B}_k^T \\bm{S}_{k+1} \\bm{B}_k,\n",
    "        $$\n",
    "\n",
    "     iv) Solve the **Bellman Equation (DPA)**, which is quadratic in $ \\bm{\\delta u}_k $, and update the coefficients in policy:\n",
    "        $$\n",
    "        \\bm{\\delta u}_{k,ff}^* = - \\bm{H}_k^{-1} \\bm{l}_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{K}_k = -\\bm{H}_k^{-1} \\bm{G}_k,\n",
    "        $$\n",
    "\n",
    "     v) Update the variance and the covariance matrixes:\n",
    "        $$\n",
    "        \\bar{s}_k = \\bar{g}_k + \\bar{s}_{k+1} + \\frac{1}{2} \\bm{\\delta u}_{k,ff}^{*T} \\bm{H}_k \\bm{\\delta u}_{k,ff}^{*} + \\bm{\\delta u}_{k,ff}^{*T} l_k,\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{s}_k = \\bm{q}_k + \\bm{A}_k^T \\bm{s}_{k+1} + \\bm{K}_k^T \\bm{H}_k \\bm{\\delta u}_{k,ff}^{*} + \\bm{K}_k^T \\bm{l}_k + \\bm{G}_k^T \\bm{\\delta u}_{k,ff}^{*},\n",
    "        $$\n",
    "        $$\n",
    "        \\bm{S}_k = \\bm{}Q_k + \\bm{A}_k^T \\bm{S}_{k+1} \\bm{A}_k + \\bm{K}_k^T \\bm{H}_k \\bm{K}_k + \\bm{K}_k^T \\bm{G}_k + \\bm{G}_k^T \\bm{K}_k,\n",
    "        $$\n",
    "\n",
    "  4. Repeat until a termination condition is satisfied and return $ \\{\\bm{\\mu}_k^l(\\bm{x}_k)\\}_{k=0}^{N-1} $.\n",
    "\n",
    "- **Optimal Policy** :\n",
    "        $$\n",
    "        \\bm{\\mu}_k^l(\\bm{x}_k) = \\bm{u}_{k,ff} + \\bm{K}_k \\bm{x}_k\n",
    "        $$\n",
    "        with $ \\bm{u}_{k,ff} = \\bm{\\bar{u}}_k - \\bm{H}_k^{-1} \\bm{l}_k $ and $ \\bm{K}_k = -\\bm{H}_k^{-1} \\bm{G}_k $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d49cc",
   "metadata": {},
   "source": [
    "**Step 1: define the setup function for iLQR controller**  \n",
    "\n",
    "In this step, we will implement the setup function `setup_external()`, which will be manually called after a initial trajectory is obtained using LQR controller introduced in the last chapter. For each iteration, you need to:\n",
    "\n",
    "1\\) Forward Pass: roll out the trajrctory forwards: apply the current policy to get the state series $\\{\\bm{x}_k^l\\}_{k=0}^{N}$ and input series $\\{\\bm{u}_k^l\\}_{k=0}^{N-1}$;  \n",
    "    Hint: you may use the method `one_step_forward()` from class `Dynamics` to compute the next state\n",
    "\n",
    "2\\) Initialize Backward Pass: initialize $\\bar{s}_k, \\bm{s}_k, \\bm{S}_k$ with $\\bar{s}_N, \\bm{s}_N, \\bm{S}_N$;  \n",
    "\n",
    "3\\) Backward Pass Loop:  retrieve the linearized $\\bm{A}$ and $\\bm{B}$ matrices from the discrete-time system dynamics on operating point;  \n",
    "    Hint: you may use the method `get_linearized_AB_discrete()` from class `Dynamics` to get the discretized system matrices\n",
    "\n",
    "4\\) Backward Pass Loop: update $\\bar{g}_k, \\bm{q}_k, \\bm{Q}_k, \\bm{r}_k, \\bm{R}_k, \\bm{P}_k, \\bm{l}_k, \\bm{G}_k, \\bm{H}_k, \\bm{\\delta u}_{k,ff}^*, \\bm{K}_k, \\bar{s}_k, \\bm{s}_k, \\bm{S}_k$ sequentially;  \n",
    "    Hint: you may use the method `numpy.linalg.inv()` to solve the inverse matrix\n",
    "\n",
    "5\\) Check on policy convergence: $max(\\{\\bm{u}_k^{l+1}\\}_{k=0}^{N-1} - \\{\\bm{u}_k^l\\}_{k=0}^{N-1}) \\leq \\Delta_{max}$ or $\\|\\{\\bm{u}_k^{l+1}\\}_{k=0}^{N-1} - \\{\\bm{u}_k^l\\}_{k=0}^{N-1}\\| \\leq \\Delta_{norm}$\n",
    "\n",
    "Note that in this implementation, for a better performance comparison with the LQR controller, the stage cost and terminal cost is still considered to be a quadratic function, and the task is considered to be a stabilization task, i.e.:\n",
    "\n",
    "$$\n",
    "g_k(\\bm{x}_k, \\bm{u}_k) = (\\bm{x}_k - \\bm{x}_{ref})^T \\bm{Q} (\\bm{x}_k - \\bm{x}_{ref}) + \\bm{u}_k^T \\bm{R} \\bm{u}_k, \\quad k \\in [0, ..., N-1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_N(\\bm{x}_N) = (\\bm{x}_N - \\bm{x}_{ref})^T \\bm{Q}_f (\\bm{x}_N - \\bm{x}_{ref})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_external(self, input_traj: np.ndarray):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform iLQR to compute the optimal control sequence.\n",
    "    -----------------------------------------------------\n",
    "    Argument: input_traj (np.ndarray), the initial control sequence (typically generated by LQR).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up equilibrium state\n",
    "    # Note that if target state is not on the slope, self.u_eq = 0 -> will not work for the nonlinear case\n",
    "    self.x_eq = self.target_state\n",
    "\n",
    "    # Solve input at equilibrium\n",
    "    self.u_eq = self.dynamics.get_equilibrium_input(self.x_eq)\n",
    "    \n",
    "    # Start iLQR\n",
    "    N = len(input_traj)\n",
    "\n",
    "    # Initialize state and control trajectories\n",
    "    x_traj = np.zeros((self.dim_states, N+1))  # State trajector\n",
    "    u_traj = np.copy(input_traj)  # Control trajectory\n",
    "    x_traj[:, 0] = self.init_state  # Initial state\n",
    "    \n",
    "    # Initialize fb and ff gain\n",
    "    self.K_k_arr = np.zeros((self.dim_states, N))\n",
    "    self.u_kff_arr = np.zeros((N))\n",
    "\n",
    "    for n in range(self.max_iter):\n",
    "\n",
    "        # Forward pass: Simulate system using current control sequence\n",
    "        for k in range(N):\n",
    "            next_state = self.dynamics.one_step_forward(current_state=x_traj[:, k], current_input=u_traj[k], dt=self.dt)\n",
    "            x_traj[:, k + 1] = next_state\n",
    "\n",
    "        # Backward pass: Compute cost-to-go and update control\n",
    "        x_N_det = x_traj[:, -1] - self.target_state\n",
    "        x_N_det = x_N_det.reshape(-1, 1) # reshape into column vector\n",
    "        #print(f\"x_N_det: {x_N_det}\")\n",
    "\n",
    "        s_k_bar = (x_N_det.T @ self.Qf @ x_N_det) / 2 # Terminal cost\n",
    "        s_k = self.Qf @ x_N_det # Terminal cost gradient\n",
    "        S_k = self.Qf # Terminal cost Hessian\n",
    "\n",
    "        for k in range(N - 1, -1, -1):\n",
    "\n",
    "            # Linearize dynamics: f(x, u) ≈ A*x + B*u\n",
    "            A_lin, B_lin = self.dynamics.get_linearized_AB_discrete(current_state=x_traj[:, k], current_input=u_traj[k], dt=self.dt)\n",
    "\n",
    "            # Compute Q matrices\n",
    "            x_k_det = x_traj[:, k] - self.target_state\n",
    "            x_k_det = x_k_det.reshape(-1, 1) # reshape into column vector\n",
    "            \n",
    "            g_k_bar = (x_k_det.T @ self.Q @ x_k_det + self.R * u_traj[k] ** 2) * self.dt / 2\n",
    "            q_k = (self.Q @ x_k_det) * self.dt\n",
    "            Q_k = (self.Q) * self.dt\n",
    "            r_k = (self.R * u_traj[k]) * self.dt\n",
    "            R_k = (self.R) * self.dt\n",
    "            P_k = np.zeros((2,)) * self.dt # should be row vector\n",
    "\n",
    "            l_k = (r_k + B_lin.T @ s_k)\n",
    "            G_k = (P_k + B_lin.T @ S_k @ A_lin) # should be row vector\n",
    "            H_k = (R_k + B_lin.T @ S_k @ B_lin)\n",
    "\n",
    "            det_u_kff = - np.linalg.inv(H_k) @ l_k\n",
    "            K_k = - np.linalg.inv(H_k) @ G_k  # should be row vector\n",
    "            u_kff = u_traj[k] + det_u_kff - (K_k @ x_traj[:, k])\n",
    "\n",
    "            self.K_k_arr[:, k] = (K_k.T).flatten()\n",
    "            self.u_kff_arr[k] = u_kff.item()\n",
    "\n",
    "            s_k_bar = g_k_bar + s_k_bar + (det_u_kff.T @ H_k @ det_u_kff) / 2 + det_u_kff.T @ l_k\n",
    "            s_k = q_k + A_lin.T @ s_k + K_k.T @ H_k @ det_u_kff + K_k.T @ l_k + G_k.T @ det_u_kff\n",
    "            S_k = Q_k + A_lin.T @ S_k @ A_lin + K_k.T @ H_k @ K_k + K_k.T @ G_k + G_k.T @ K_k\n",
    "            \n",
    "        # Update control sequence\n",
    "        new_u_traj = np.zeros_like(u_traj)\n",
    "        new_x_traj = np.zeros_like(x_traj)\n",
    "        new_x_traj[:, 0] = self.init_state\n",
    "        \n",
    "        # Simulation forward to get input sequence\n",
    "        for k in range(N):\n",
    "            new_u_traj[k] = self.u_kff_arr[k] + self.K_k_arr[:, k].T @ new_x_traj[:, k]\n",
    "            next_state = self.dynamics.one_step_forward(current_state=new_x_traj[:, k], current_input=new_u_traj[k], dt=self.dt)\n",
    "            new_x_traj[:, k + 1] = next_state\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.max(np.abs(new_u_traj - u_traj)) < self.tol:\n",
    "            print(f\"Use {n} iteration until converge.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Iteration {n}: residual error is {np.max(np.abs(new_u_traj - u_traj))}\")\n",
    "            #print(f\"Old input trajectory: {u_traj.flatten()}\")\n",
    "            #print(f\"New input trajectory: {new_u_traj.flatten()}\")\n",
    "\n",
    "        u_traj = new_u_traj\n",
    "        x_traj = new_x_traj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6160",
   "metadata": {},
   "source": [
    "**Step 2: Bind the defined setup function to the class \"iLQRController\", and run the simulation to see the performance of controller**  \n",
    "\n",
    "1\\) Bind the defined setup function `setup_external()` to class `iLQRController`;\n",
    "\n",
    "2\\) Specify the arguments and instantiate the controller class `LQRController` as `LQR`; \n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "    i) weight for state $\\bm{Q} = \\bm{Q}_f = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\bm{R} = [1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "3\\) Instantiate the class `Simulator` for LQR and call function `run_simulation()` and `get_trajectories()` to get the simulated state- and input-trajectory;\n",
    "\n",
    "4\\) Instantiate the controller class `iLQRController` as `iLQR_0` and call function `setup()` to generate the optimal iLQR policy; \n",
    "\n",
    "5\\) Instantiate the class `Simulator` for iLQR and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "6\\) Instantiate the class `Visualizor` for iLQR, call function `display_final_results()` and `display_animation()` to show the simulations;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68387edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the defined DP algorithm to the corresponding class\n",
    "iLQRController.setup = setup_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f87f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 6 # time length of simulation\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the LQR controller class to initialize iLQR controller\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_lqr = LQRController(env, dynamics, Q, R, freq, name='LQR')\n",
    "\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr = Simulator(dynamics, controller_lqr, env, 1/freq, t_terminal)\n",
    "simulator_lqr.run_simulation()\n",
    "\n",
    "# Use trajectories getting from LQR to initialize iLQR\n",
    "_, input_traj_lqr = simulator_lqr.get_trajectories()\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the iLQR controller class\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state in stage cost, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input in stage cost, type: np.array  \n",
    "#                                         iii) `Qf`: weight metrix of state in terminal cost, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) name: the name of current coltroller displayed in plots, type: string\n",
    "controller_ilqr_0 = iLQRController(env, dynamics, Q, R, Qf, freq, name='iLQR_0')\n",
    "\n",
    "# Initialize iLQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_0.setup(input_traj_lqr)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_0 = Simulator(dynamics, controller_ilqr_0, env, 1/freq, t_terminal)\n",
    "simulator_ilqr_0.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_0 = Visualizer(simulator_ilqr_0)\n",
    "visualizer_ilqr_0.display_plots()\n",
    "#visualizer_ilqr_0.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176c67a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c949",
   "metadata": {},
   "source": [
    "### **Part (C): LQR v.s. iLQR**\n",
    "\n",
    "In this chapter, we will explore several aspects of how the iLQR algorithm compares to the LQR algorithm. We will begin by examining the relationship between LQR and iLQR under both linear and nonlinear system dynamics. Then we will delve deeper into the advantages of iLQR especially in the nonlinear case and provide visualizations to enhance understanding.  \n",
    "\n",
    "#### **1. Relationship between LQR and iLQR**\n",
    "\n",
    "In this section, we will explore the relationship between LQR and iLQR through a comparative study. We design two test cases:  \n",
    "\n",
    " - `xxx_linear`: Using a purely linear system for the dynamics (EG: case 1)\n",
    "\n",
    " - `xxx_nonlinear`: Using a nonlinear system dynamics (EG: case 4)\n",
    "\n",
    "while keeping the same task definitions and cost design as in the linear case. By analyzing the results, we aim to understand how nonlinearity impacts the effectiveness of the LQR controller under identical conditions. You need to:  \n",
    "\n",
    "1\\) Define the parameters in stage and terminal cost and for simulation;\n",
    "\n",
    "2\\) For the linear case: Specify the case index as 1, instantiate classes `Env`, `Dynamcis`, `LQRController`, `iLQRController`, `Simulator` and `Visualizer`, and call function `display_contrast_plots()` to see the difference between LQR and iLQR policy; \n",
    "\n",
    "3\\) For the nonlinear case: Specify the case index as 4, instantiate classes `Env`, `Dynamcis`, `LQRController`, `iLQRController`, `Simulator` and `Visualizer`, and call function `display_contrast_plots()` to see the difference between LQR and iLQR policy; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 6 # time length of simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear case\n",
    "# Define the index for linear case\n",
    "case_linear = 1 # 1 or 2\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_linear = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_linear.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_linear = Dynamics(env_linear)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize iLQR controller\n",
    "controller_lqr_linear = LQRController(env_linear, dynamics_linear, Q, R, freq, name='LQR_linear')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear = Simulator(dynamics_linear, controller_lqr_linear, env_linear, 1/freq, t_terminal)\n",
    "simulator_lqr_linear.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize iLQR\n",
    "_, input_traj_lqr_linear = simulator_lqr_linear.get_trajectories()\n",
    "\n",
    "# Instantiate the iLQR controller class\n",
    "controller_ilqr_linear = iLQRController(env_linear, dynamics_linear, Q, R, Qf, freq, name='iLQR_linear')\n",
    "# Initialize iLQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_linear.setup(input_traj_lqr_linear)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_linear = Simulator(dynamics_linear, controller_ilqr_linear, env_linear, 1/freq, t_terminal)\n",
    "simulator_ilqr_linear.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_linear = Visualizer(simulator_ilqr_linear)\n",
    "visualizer_ilqr_linear.display_contrast_plots(simulator_lqr_linear)\n",
    "#visualizer_ilqr_linear.display_contrast_animation(simulator_lqr_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear case\n",
    "# Define the index for linear case\n",
    "case_nonlinear_0 = 4\n",
    "\n",
    "initial_position = 0.0\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_0 = Env(case_nonlinear_0, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_nonlinear_0.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear_0 = Dynamics(env_nonlinear_0)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize iLQR controller\n",
    "controller_lqr_nonlinear_0 = LQRController(env_nonlinear_0, dynamics_nonlinear_0, Q, R, freq, name='LQR_nonlinear_0')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_0 = Simulator(dynamics_nonlinear_0, controller_lqr_nonlinear_0, env_nonlinear_0, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_0.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize iLQR\n",
    "_, input_traj_lqr_nonlinear_0 = simulator_lqr_nonlinear_0.get_trajectories()\n",
    "\n",
    "# Instantiate the iLQR controller class\n",
    "controller_ilqr_nonlinear_0 = iLQRController(env_nonlinear_0, dynamics_nonlinear_0, Q, R, Qf, freq, name='iLQR_nonlinear_0')\n",
    "# Initialize iLQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_0.setup(input_traj_lqr_nonlinear_0)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear_0 = Simulator(dynamics_nonlinear_0, controller_ilqr_nonlinear_0, env_nonlinear_0, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_0.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_0 = Visualizer(simulator_ilqr_nonlinear_0)\n",
    "visualizer_ilqr_nonlinear_0.display_contrast_plots(simulator_lqr_nonlinear_0)\n",
    "#visualizer_ilqr_nonlinear_0.display_contrast_animation(simulator_lqr_nonlinear_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671287ca",
   "metadata": {},
   "source": [
    "#### **Results Analysis:**\n",
    "\n",
    "From the results shown in the figure, a significant difference can be observed when applying the LQR control law to a system with nonlinear dynamics versus a linear system:  \n",
    "\n",
    "- **Linear Dynamics** (`LQR_linear`, `iLQR_linear`): In the linear-quadratic setting, where system dynamics are strictly linear (case 1 or case 2) and the cost function is purely quadratic, iLQR coincides with the LQR solution, leading to the same optimal policy.  \n",
    "\n",
    "- **Nonlinear Dynamics** (`LQR_nonlinear`, `iLQR_nonlinear`): However, when the system dynamics become nonlinear (case 3 or case 4), iLQR adapts by iteratively approximating both the dynamics and the cost around a nominal trajectory, where LQR can only linearize the system dynamics around a single operating point. From the position curve we can also observe that the LQR curve exhibits tiny overshoot when the iLQR controller can drivce the car gradually approach the target position.\n",
    "\n",
    "#### **Main Conclusion:**\n",
    "\n",
    "Compared with the LQR algorithm, this iterative procedure allows for finding a locally optimal policy that is no longer captured by classical LQR. In this sense, iLQR serves as a more general framework, reducing to LQR in the simpler linear-quadratic case but extending to broader classes of nonlinear systems and more complex costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055c152",
   "metadata": {},
   "source": [
    "\n",
    "#### **2. How does iLQR outperform LQR in a nonlinear case**\n",
    "\n",
    "In the last section we have already proved that LQR policy is just a special case of iLQR policy for the linear dynamics and the quadratic cost function. For the rest cases, iLQR will lead to a more optimal policy comparing with the LQR controller. The optimality here is subject to the optimizatioin problem, which is determined by all the components (dynamcis, cost, and constraints). In this section, we're going provide you with some computational results to further address this consequece. You need to:  \n",
    "\n",
    "1\\) Define the index for nonlinear test case and the parameters for teh simulation;\n",
    "\n",
    "2\\) For LQR and iLQR: Instantiate classes `Env`, `Dynamcis`, `LQRController`, `iLQRController` and `Simulator`; \n",
    "\n",
    "3\\) For LQR and iLQR: call function `compute_cost2go()` to evaluate the cost-to-go from backwards;\n",
    "\n",
    "4\\) Call function `display_contrast_cost2go()` to visualize the comparison between the LQR cost and the iLQR cost w.r.t. time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nonlinear case\n",
    "# Define the index for linear case\n",
    "case_nonlinear_1 = 4 \n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_1 = Env(case_nonlinear_1, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "#env_nonlinear_1.test_env()\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear_1 = Dynamics(env_nonlinear_1)\n",
    "\n",
    "# Instantiate the LQR controller class to initialize iLQR controller\n",
    "controller_lqr_nonlinear_1 = LQRController(env_nonlinear_1, dynamics_nonlinear_1, Q, R, freq, name='LQR_nonlinear_1')\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear_1 = Simulator(dynamics_nonlinear_1, controller_lqr_nonlinear_1, env_nonlinear_1, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear_1.run_simulation()\n",
    "\n",
    "# Use trajectories getting from LQR to initialize iLQR\n",
    "_, input_traj_lqr_nonlinear_1 = simulator_lqr_nonlinear_1.get_trajectories()\n",
    "\n",
    "# Instantiate the iLQR controller class\n",
    "controller_ilqr_nonlinear_1 = iLQRController(env_nonlinear_1, dynamics_nonlinear_1, Q, R, Qf, freq, name='iLQR_nonlinear_1')\n",
    "# Initialize iLQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_1.setup(input_traj_lqr_nonlinear_1)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear_1 = Simulator(dynamics_nonlinear_1, controller_ilqr_nonlinear_1, env_nonlinear_1, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_1.run_simulation()\n",
    "\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_lqr_nonlinear_1 = simulator_lqr_nonlinear_1.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "\n",
    "#print(f\"Cost-to-go of LQR controller: {cost2go_lqr_nonlinear_1[0]}\")\n",
    "# Call rollout function to compute the cost-to-go\n",
    "cost2go_ilqr_nonlinear_1 = simulator_ilqr_nonlinear_1.compute_cost2go(Q, R, Qf, np.array([target_position, target_velocity]))\n",
    "#print(f\"Cost-to-go of iLQR controller: {cost2go_ilqr_nonlinear_1[0]}\")\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_1 = Visualizer(simulator_ilqr_nonlinear_1)\n",
    "visualizer_ilqr_nonlinear_1.display_contrast_cost2go(simulator_lqr_nonlinear_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b6d926",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "From the results, we observe that in the case of nonlinear dynamics, the policy computed by iLQR yields a lower cost-to-go compared to the LQR policy, indicating a more optimal solution. This is because iLQR better accounts for system nonlinearity during the optimization process, rather than relying on a single linearization around a reference point.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb8cdb",
   "metadata": {},
   "source": [
    "#### **Wrap-up:**\n",
    "\n",
    "In this chapter, we introduced the principles of the iLQR controller, provided a simple implementation, and visualized its performance in comparison with the LQR controller. The results demonstrate that iLQR is better suited for handling:\n",
    "\n",
    "- Nonlinearty in system dynamcis\n",
    "- Non-quadratic cost function\n",
    "\n",
    "However, as the cost, iLQR requires greater computational effort and is typically limited to offline computation. Similar to LQR, it also do not support explicit definition of the state- and input-constraints. In the following chapters, we will introduce the Model Predictive Control (MPC), which approximates an infinite-horizon optimal control problem with a finite and receding horizon formulation. This enables constraint handling and significantly broadens the applicability of optimal control method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
